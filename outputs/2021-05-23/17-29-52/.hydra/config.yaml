core:
  version: 0.0.1
  tags:
  - mnist
datamodule:
  _target_: src.pl_data.datamodule.MyDataModule
  val_percentage: 0.15
  datasets:
    train:
      _target_: src.pl_data.dataset.MyDataset
      name: MNIST_TRAIN
      train: true
      path: ${oc.env:MNIST}
    test:
    - _target_: src.pl_data.dataset.MyDataset
      name: MNIST_TEST
      train: false
      path: ${oc.env:MNIST}
  num_workers:
    train: 8
    val: 4
    test: 4
  batch_size:
    train: 32
    val: 16
    test: 16
run:
  dir: .cache/${now:%Y-%m-%d}/${now:%H-%M-%S}
sweep:
  dir: .cache/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}/
  subdir: ${hydra.job.num}_${hydra.job.id}
job:
  env_set:
    WANDB_START_METHOD: thread
    WANDB_DIR: ${oc.env:PROJECT_ROOT}
n_elements_to_log: 64
val_check_interval: 1.0
progress_bar_refresh_rate: 20
wandb:
  project: gnn-learning
  entity: giuliacassara
  log_model: true
  mode: online
wandb_watch:
  log: all
  log_freq: 100
lr_monitor:
  logging_interval: step
  log_momentum: false
_target_: src.pl_modules.model.MyModel
optimizer:
  _target_: torch.optim.Adam
  lr: 0.001
  betas:
  - 0.9
  - 0.999
  eps: 1.0e-08
  weight_decay: 0
use_lr_scheduler: true
lr_scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
  T_0: 10
  T_mult: 2
  eta_min: 0
  last_epoch: -1
  verbose: true
deterministic: false
random_seed: 42
pl_trainer:
  fast_dev_run: false
  gpus: 1
  precision: 32
  max_steps: 1000
  accumulate_grad_batches: 8
  num_sanity_val_steps: 2
  gradient_clip_val: 10.0
monitor_metric: val_loss
monitor_metric_mode: min
early_stopping:
  patience: 42
  verbose: false
model_checkpoints:
  save_top_k: 2
  verbose: false
